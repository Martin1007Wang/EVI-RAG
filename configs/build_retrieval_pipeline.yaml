# Explicitly provide `dataset=<name>` and `paths=<name>` via CLI overrides.

# Shared encoder settings (used by both parquet + LMDB stages).
encoder: "Alibaba-NLP/gte-large-en-v1.5"
device: "cuda"
batch_size: 2048
progress_bar: true
fp16: false

# === Stage 1: normalize raw parquet into retrieval parquet ===
dataset_name: ${dataset.name}
kb: ${dataset.kb}
raw_root: ${dataset.raw_root}
out_dir: ${dataset.out_dir}
column_map: ${dataset.column_map}
entity_normalization: ${dataset.entity_normalization}
entity_text_mode: ${dataset.entity_text_mode}
text_prefixes: ${dataset.text_prefixes}
text_regex: ${dataset.text_regex}
path_mode: undirected

embeddings_out_dir: ${dataset.materialized_dir}/embeddings
precompute_entities: true
precompute_relations: true
precompute_questions: true
canonicalize_relations: false
cosine_eps: 1e-6

emit_sub_filter: true
sub_filter_filename: sub_filter.json

# === Stage 2: materialize LMDBs from normalized parquet ===
parquet_dir: ${dataset.out_dir}
output_dir: ${dataset.materialized_dir}
use_precomputed_embeddings: true
use_precomputed_questions: true
require_precomputed_questions: true
num_topics: ${dataset.num_topics}

map_size_gb: 32
vocab_map_size_gb: 4
overwrite_lmdb: true
txn_size: 512
seed: 42
deterministic: false
