# @package _global_
#
# Energy-based GFlowNet training (EB-GFN).

defaults:
  - override /data: g_retrieval
  - override /model: gflownet_module
  - override /callbacks: default
  - override /logger: wandb
  - override /trainer: gpu

task_name: "eb_gfn_train"
tags: ["gflownet", "energy"]
train: true
test: true
ckpt_path: null
seed: 42
optimized_metric: "val/terminal_hit@${model.evaluation_cfg.num_eval_rollouts}"

trainer:
  check_val_every_n_epoch: 5

data:
  expand_multi_start: false
  expand_multi_answer: false

model:
  env:
    max_steps: ${dataset.max_steps}

  selector_cfg:
    enabled: true
    epsilon: 0.1

  optimizer_cfg:
    type: adamw
    lr: 1.0e-4
    weight_decay: 1.0e-4
    param_groups:
      - names: ["*log_f.*", "*log_f_backward.*"]
        lr: 5.0e-4

  reward_fn:
    hard_target_bonus: 2.0
    min_log_reward: -5.0

  scheduler_cfg:
    type: cosine
    t_max: 10
    eta_min: 1.0e-6
    interval: epoch

  actor_cfg:
    policy_temperature: 1.2

  backward_cfg:
    share_state_encoder: false
    share_edge_scorer: false

  training_cfg:
    num_train_rollouts: 8
    rollout_chunk_size: 4
    accumulate_grad_batches: 4
    manual_gradient_clip_val: 1.0
    manual_gradient_clip_algorithm: norm
    manual_gradient_clip_norm_type: 2.0
    adaptive_gradient_clip: true
    grad_clip_ema_beta: null
    grad_clip_tail_prob: null
    grad_clip_log_eps: 1.0e-8
    grad_clip_tail_prob_eps: 1.0e-6
    grad_clip_log_interval: 10
    grad_non_finite_max_params: 20
    subtb:
      enabled: true
      num_subtrajectories: 4
    imitation:
      # Online mutual imitation / distillation (no-cache replay).
      enabled: false
      weight: 1.0
    replay:
      # Deprecated: replay buffer removed. Use `training_cfg.imitation` instead.
      enabled: false
      buffer_size: 4096
      mix_ratio: 0.5
    dual_stream:
      enabled: true
      stream_forward_weight: 1.0
      stream_forward_max_steps: null
      stream_forward_weight_start: 0.0
      stream_forward_weight_schedule: none
      stream_forward_weight_anneal_epochs: null
    z_align:
      enabled: false
      weight: 10.0
    h_guidance:
      enabled: true
      beta_start: 0.0
      beta_end: 1.0
      warmup_progress: 0.1
      apply_eval: true
      stop_gradient: true
      scale: 1.0
    target_sampling:
      # 多答案时为 backward rollout 采样单一起点，避免多起点报错。
      mode: random_one

callbacks:
  model_checkpoint:
    monitor: val/terminal_hit@${model.evaluation_cfg.num_eval_rollouts}
    mode: max
    filename: "epoch_{epoch:03d}"
    auto_insert_metric_name: False
    save_weights_only: True
  early_stopping:
    monitor: val/terminal_hit@${model.evaluation_cfg.num_eval_rollouts}
    mode: max
    patience: 50
