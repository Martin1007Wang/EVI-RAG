_target_: src.models.reasoner_module.ReasonerModule

mode: llm

# Ollama-specific LLM config (local HTTP backend)
model_name: "llama3:8b"     # replace with your pulled Ollama model tag
backend: ollama
ollama_base_url: "http://localhost:11434"
ollama_timeout: 120.0

# Shared generation params
tensor_parallel_size: 1              # ignored by Ollama
temperature: 0.0
frequency_penalty: 0.16
max_seq_len: 16384
max_tokens: 4000
seed: ${seed}
output_dir: ${paths.output_dir}

dataset: ${data.dataset}
split: ${data.split}
prompt_tag: ${data.prompt_tag}
