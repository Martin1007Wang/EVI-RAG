_target_: src.models.llm_reasoner_module.LLMReasonerModule

model_name: meta-llama/Meta-Llama-3.1-8B-Instruct
tensor_parallel_size: 1
temperature: 0.0
frequency_penalty: 0.16
max_seq_len: 16384
max_tokens: 4000
seed: ${seed}
output_dir: ${paths.output_dir}

dataset: ${data.dataset}
split: ${data.split}
prompt_tag: ${data.prompt_tag}

# Backend selection: auto | vllm | openai | ollama
backend: auto
ollama_base_url: "http://localhost:11434"
ollama_timeout: 120.0
