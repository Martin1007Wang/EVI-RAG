defaults:
  - _self_
  - env@env_cfg: graph
  - policy@policy_cfg: frontier
  - reward@reward_cfg: answer_pos_f1
  - actor@actor_cfg: gflownet_actor
  - embedder@embedder_cfg: default
  - state_encoder@state_encoder_cfg: gnn
  - estimator@estimator_cfg: gflownet_estimator

_target_: src.models.gflownet_module.GFlowNetModule
_recursive_: false

hidden_dim: 1024

training_cfg:
  debug_batches_to_log: 1
  debug_graphs_to_log: 2
  subtb_penalty: log1p_l2
  num_train_rollouts: 1
  # Behavior cloning on deterministic GT path (teacher forcing).
  # Set bc_coef>0 to enable; keep it 0 for pure GFlowNet training.
  bc_coef: 0.0
  bc_include_stop: false
  # Optional decay schedule for bc_coef (applied only during training).
  # NOTE: keys must exist for Hydra struct-mode overrides (so CLI can set bc_schedule.type without '+').
  bc_schedule:
    type: cosine # linear|cosine
    final_coef: 0.0
    start_frac: 0.0 # when decay starts (0..1 of total steps)
    duration_frac: 1.0 # decay duration (0..1 of total steps)
  # Teacher-forced SubTB anchor on GT trajectories (flow objective, not cross-entropy).
  # Useful when on-policy success is too sparse; keep this >0 even when bc_coef decays.
  gt_flow_coef: 0.0
  gt_flow_schedule:
    type: cosine # linear|cosine
    final_coef: 0.0
    start_frac: 0.0
    duration_frac: 1.0

evaluation_cfg:
  num_eval_rollouts: 5
  rollout_temperature: 1.0

optimizer_cfg:
  type: adamw
  lr: 1.0e-4
  weight_decay: 1.0e-4

scheduler_cfg:
  type: cosine
  t_max: 200
  eta_min: 1.0e-6
  interval: epoch
  monitor: val/answer_hit@1

logging_cfg:
  train_prog_bar: ["log_reward", "answer_hit", "answer_reach_frac"]
  eval_prog_bar: ["answer_hit@1", "answer_reach_frac"]
  log_on_step_train: false
